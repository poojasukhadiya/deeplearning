# -*- coding: utf-8 -*-
"""deeplearning1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nT1yj85GXSZP8QrPsdqKrsX96k0W0iWb
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import DataLoader, TensorDataset

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import torch
from torch.utils.data import DataLoader, TensorDataset

import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Load the dataset
data = pd.read_csv('/content/Dataset1_labeled_data.csv')
# Print the first few rows of the DataFrame
print(data.head())

import matplotlib.pyplot as plt

# Assuming 'class' is a column in your DataFrame 'data'
plt.figure(figsize=(8, 5))
plt.hist(data['class'], bins=range(min(data['class']), max(data['class']) + 1), alpha=0.7, rwidth=0.85)
plt.title('Distribution of Classes')
plt.xlabel('Class')
plt.ylabel('Number of Samples')
plt.grid(axis='y', alpha=0.75)
plt.show()

from collections import Counter
import itertools

# Tokenize the tweets
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(data['tweet'])
sequences = tokenizer.texts_to_sequences(data['tweet'])

# Flatten the list of sequences and count word frequencies
word_counts = Counter(itertools.chain(*sequences))

# Prepare data for plotting
most_common = word_counts.most_common(20)
words = [tokenizer.index_word[i[0]] for i in most_common]
frequencies = [i[1] for i in most_common]

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x=frequencies, y=words)
plt.title('Top 20 Most Common Words')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()

# Calculate the length of each tweet
tweet_lengths = [len(tweet.split()) for tweet in data['tweet']]

# Plot
plt.figure(figsize=(10, 6))
sns.histplot(tweet_lengths, bins=30)
plt.title('Distribution of Tweet Lengths')
plt.xlabel('Length of Tweet')
plt.ylabel('Number of Tweets')
plt.show()

print(data.columns)

# Preprocess the text data
max_words = 10000  # Number of words to consider
max_len = 100      # Maximum length of sequences

# Tokenize and pad text
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data['tweet'])
sequences = tokenizer.texts_to_sequences(data['tweet'])
padded_sequences = pad_sequences(sequences, maxlen=max_len)

# Preprocess the labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(data['offensive_language'])
labels = torch.tensor(labels)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.long)
X_val_tensor = torch.tensor(X_val, dtype=torch.long)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)

# Create DataLoader for training data
train_data = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)

# Define the LSTM model
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.embedding(text)
        lstm_out, _ = self.lstm(embedded)
        hidden = lstm_out[:, -1, :]
        output = self.fc(hidden)
        return output

# Instantiate the model
model = LSTMClassifier(max_words, 128, 64, len(label_encoder.classes_))

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
for epoch in range(10):
    for texts, labels in train_loader:
        # Forward pass
        outputs = model(texts)
        loss = criterion(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')

# Create DataLoader for validation data
val_data = TensorDataset(X_val_tensor, y_val_tensor)
val_loader = DataLoader(val_data, batch_size=32, shuffle=True)

# Set the model to evaluation mode
model.eval()

# Track variables for accuracy calculation
total = 0
correct = 0

with torch.no_grad():  # No need to track gradients during evaluation
    for texts, labels in val_loader:
        # Forward pass
        outputs = model(texts)

        # Get predictions from the maximum value
        _, predicted = torch.max(outputs.data, 1)

        # Total number of labels
        total += labels.size(0)

        # Total correct predictions
        correct += (predicted == labels).sum().item()

# Calculate accuracy
accuracy = 100 * correct / total
print(f'Accuracy on the validation set: {accuracy:.2f}%')

# Tokenize and pad text
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data['tweet'])
sequences = tokenizer.texts_to_sequences(data['tweet'])
padded_sequences = pad_sequences(sequences, maxlen=max_len)

class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(BiLSTMClassifier, self).__init__()
        self.hidden_dim = hidden_dim  # Store as an instance variable
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply hidden_dim by 2 for bidirectional

    def forward(self, text):
        embedded = self.embedding(text)
        lstm_out, _ = self.lstm(embedded)
        # Use self.hidden_dim to access the hidden dimension
        hidden = torch.cat((lstm_out[:, -1, :self.hidden_dim], lstm_out[:, 0, self.hidden_dim:]), dim=1)
        output = self.fc(hidden)
        return output

# Instantiate the model
model = BiLSTMClassifier(max_words, 128, 64, len(label_encoder.classes_))

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Set the model to training mode
model.train()

# Number of epochs
num_epochs = 10

for epoch in range(num_epochs):
    total_loss = 0

    for texts, labels in train_loader:
        # Forward pass
        outputs = model(texts)
        loss = criterion(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    # Print average loss for the epoch
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')

for epoch in range(num_epochs):
    # Training phase
    model.train()  # Set the model to training mode
    total_loss = 0

    for texts, labels in train_loader:
        # Forward pass
        outputs = model(texts)
        loss = criterion(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')

    # Validation phase
    model.eval()  # Set the model to evaluation mode
    total = 0
    correct = 0

    with torch.no_grad():
        for texts, labels in val_loader:
            outputs = model(texts)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f'Validation Accuracy: {accuracy:.2f}%')

# Constants for text preprocessing
max_words = 10000  # Number of words to consider from the dataset
max_len = 50  # Maximum length of each sequence

# Tokenizing the text
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data['tweet'])
sequences = tokenizer.texts_to_sequences(data['tweet'])

# Padding the sequences
tweets_padded = pad_sequences(sequences, maxlen=max_len)

# Preparing the target variable
labels = to_categorical(data['class'])

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tweets_padded, labels, test_size=0.2, random_state=42)

# Build the RNN model
rnn_model = Sequential()
rnn_model.add(Embedding(max_words, 128, input_length=max_len))
rnn_model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2))
rnn_model.add(Dense(labels.shape[1], activation='softmax'))

# Compile the model
rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
rnn_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)

# Predictions
rnn_predictions = rnn_model.predict(X_test)
rnn_predictions = rnn_predictions.argmax(axis=1)

# Convert y_test back to labels if it's one-hot encoded
y_test_labels = np.argmax(y_test, axis=1)

# Confusion matrix for RNN
rnn_conf_matrix = confusion_matrix(y_test_labels, rnn_predictions)

# Function to plot confusion matrix
def plot_confusion_matrix(cm, model_name):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')
    plt.show()

# Plot the confusion matrix
plot_confusion_matrix(rnn_conf_matrix, 'RNN')

rnn_model.save('rnn_model.h5')  # Saves the model to a file

import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the model
model = load_model('rnn_model.h5')

# Assuming max_words and max_len are defined as they were during training
tokenizer = Tokenizer(num_words=max_words)  # Initialize tokenizer as done during training

# Function to preprocess new input text
def preprocess_text(text, max_len):
    sequences = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(sequences, maxlen=max_len)
    return padded

# Function to make predictions
def predict_sentiment(text):
    processed_text = preprocess_text(text, max_len)
    prediction = model.predict(processed_text)
    return np.argmax(prediction, axis=1)

# Example usage
sample_text = "Your sample text goes here"
predicted_label = predict_sentiment(sample_text)
print(f"Predicted label: {predicted_label}")

